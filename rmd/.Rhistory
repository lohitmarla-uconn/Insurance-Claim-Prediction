(conf_matrix_rf[1,2] + conf_matrix_rf[2,1])/sum(conf_matrix_rf)
library(ggplot2)
library(caret)
library(dplyr)
library(vip)
library(ranger)
library(rpart)
library(rpart.plot)
library(pROC)
options(warn = -1)
#Reading the csv file
insurance.data.dup <- read.csv("~/Documents/GitHub/GitHub/Insurance-Claim-Prediction/data/insurance.csv")
insurance.data <- insurance.data.dup
str(insurance.data)
nrow(insurance.data)
insurance.data <- unique(insurance.data)
nrow(insurance.data)
print("sex")
table(insurance.data$sex)
print("children")
table(insurance.data$children)
print("smoker")
table(insurance.data$smoker)
print("region")
table(insurance.data$region)
table(insurance.data$insuranceclaim)
sapply(insurance.data, function(x) unique(x))
column_names <- c(
"sex", "children", "smoker", "insuranceclaim", "region"
)
# Convert the selected columns to factors in your data frame
insurance.data[, column_names] <- lapply(insurance.data[, column_names], as.factor)
str(insurance.data)
any(is.na(insurance.data))
colSums(is.na(insurance.data)) > 0
percentage_data <- table(insurance.data$insuranceclaim) / nrow(insurance.data) * 100
# Create a data frame for plotting
plot_data <- data.frame(insuranceclaim = as.factor(names(percentage_data)),
percentage = as.numeric(percentage_data))
# Plotting
ggplot(plot_data, aes(x = insuranceclaim, y = percentage)) +
geom_bar(stat = "identity", fill = "skyblue", color = "black") +
geom_text(aes(label = sprintf("%.1f%%", percentage)),
position = position_stack(vjust = 0.5),   # Adjust vjust for vertical position
color = "black", size = 3) +
labs(title = "Distribution of Insurance Claims: Non-Claims (0) vs. Claims (1)",
x = "Insurance Claim",
y = "Percentage")
# Binomial Logit Model - 80-20 split
set.seed(123457)
train.prop <- 0.80
strats <- insurance.data$insuranceclaim
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr,
function(x) sample(x, length(x)*train.prop)))))
insurance.data.train <- insurance.data[idx, ]
insurance.data.test <- insurance.data[-idx, ]
#check for equal proportions of number of claims
table(insurance.data.train$insuranceclaim)/nrow(insurance.data.train)
table(insurance.data.test$insuranceclaim)/nrow(insurance.data.test)
#full binary logit model
full.logit <- glm(insuranceclaim ~ . ,data = insurance.data.train,
family = binomial(link = "logit"))
summary(full.logit)
car::qqPlot(residuals(full.logit), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(full.logit))
#null binary logit model
null.logit <- glm(insuranceclaim ~ 1 ,data = insurance.data.train,
family = binomial(link = "logit"))
summary(null.logit)
car::qqPlot(residuals(null.logit), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(null.logit))
both.logit <- step(null.logit, list(lower = formula(null.logit),
upper = formula(full.logit)),
direction = "both", trace = 0, data = insurance.data.train)
formula(both.logit)
summary(both.logit)
car::qqPlot(residuals(both.logit), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(both.logit))
extpts <- which(abs(residuals(both.logit)) > 3*sd(residuals(both.logit)))
nrow(insurance.data.train)
length(extpts)
data.train.2 <- insurance.data.train[-extpts,]
full.logit <- glm(insuranceclaim ~ . ,data = data.train.2,
family = binomial(link = "logit"))
both.logit.extpts <- step(full.logit,
direction="both",trace=0, data = data.train.2)
formula(both.logit.extpts)
summary(both.logit.extpts)
car::qqPlot(residuals(both.logit.extpts), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(both.logit.extpts))
#Akaike Information Criterion
AIC(both.logit)
AIC(full.logit)
AIC(null.logit)
#Baysian Information Criteria
BIC(both.logit)
BIC(full.logit)
BIC(null.logit)
pred.both.test <- predict(both.logit, newdata = insurance.data.test, type="response")
pred.full.test <- predict(full.logit, newdata = insurance.data.test, type="response")
(table.both <- table(pred.both.test > 0.5, insurance.data.test$insuranceclaim))
(table.full <- table(pred.full.test > 0.5, insurance.data.test$insuranceclaim))
(accuracy.both <- round((sum(diag(table.both))/sum(table.both))*100,2))
(accuracy.full <- round((sum(diag(table.full))/sum(table.full))*100,2))
par(mfrow = c(1,2))
roc.both <- roc(insurance.data.test$insuranceclaim ~ pred.both.test, plot = TRUE,
legacy.axes = TRUE, print.auc = TRUE)
roc.both <- roc(insurance.data.test$insuranceclaim ~ pred.full.test, plot = TRUE,
legacy.axes = TRUE, print.auc = TRUE)
pred.both <- predict(both.logit, newdata = insurance.data.train, type="response")
pred.full <- predict(full.logit, newdata = insurance.data.train, type="response")
(table.both <- table(pred.both > 0.5, insurance.data.train$insuranceclaim))
(table.full <- table(pred.full > 0.5, insurance.data.train$insuranceclaim))
(accuracy.both <- round((sum(diag(table.both))/sum(table.both))*100,2))
(accuracy.full <- round((sum(diag(table.full))/sum(table.full))*100,2))
par(mfrow = c(1,2))
roc.both <- roc(insurance.data.train$insuranceclaim ~ pred.both, plot = TRUE,
legacy.axes = TRUE, print.auc = TRUE)
roc.both <- roc(insurance.data.train$insuranceclaim ~ pred.full, plot = TRUE,
legacy.axes = TRUE, print.auc = TRUE)
both.logit.backward <- step(full.logit,
direction="backward",trace=0, data = insurance.data.train)
formula(both.logit.backward)
summary(both.logit.backward)
car::qqPlot(residuals(both.logit.backward), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(both.logit.backward))
both.logit.forward <- step(full.logit,
direction="forward",trace=0, data = insurance.data.train)
formula(both.logit.forward)
summary(both.logit.forward)
car::qqPlot(residuals(both.logit.forward), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(both.logit.forward))
pred.both.forward <- predict(both.logit.forward, newdata = insurance.data.test, type="response")
pred.both.backward <- predict(both.logit.backward, newdata = insurance.data.test, type="response")
(table.both.forward <- table(pred.both.forward > 0.5, insurance.data.test$insuranceclaim))
(table.full.backward <- table(pred.both.backward > 0.5, insurance.data.test$insuranceclaim))
(accuracy.both.forward <- round((sum(diag(table.both.forward))/sum(table.both.forward))*100,2))
(accuracy.full.backward <- round((sum(diag(table.full.backward))/sum(table.full.backward))*100,2))
# Binomial Full Logit Model - K fold validaton
# Set the number of folds (K)
num_folds <- 10
# Create an index vector for splitting
set.seed(123)  # for reproducibility
indices <- createFolds(insurance.data$insuranceclaim, k = num_folds, list = TRUE)
# Initialize a variable to store cross-validation results
cv_results <- data.frame(Accuracies = double(num_folds))
# Perform K-Fold Cross-Validation
for (i in 1:num_folds) {
# Split the data into training and testing sets
train_data <- insurance.data[-indices[[i]], ]
test_data <- insurance.data[indices[[i]], ]
# Fit your model on the training data
model <- glm(insuranceclaim ~ . ,data = train_data,
family = binomial(link = "logit"))
# Make predictions on the test data
predictions <- predict(model, newdata = test_data,  type="response")
(table.full <- table(predictions > 0.5, test_data$insuranceclaim))
acc <- round((sum(diag(table.full))/sum(table.full))*100,2)
# Store the RMSE in the results dataframe
cv_results$Accuracies[i] <- acc
}
# Display cross-validation results
print(cv_results)
cv_results$Accuracies <- as.numeric(cv_results$Accuracies)
mean_accuracy <- mean(cv_results$Accuracies, na.rm = TRUE)
mean_accuracy
# Binomial Logit Model - K fold validation
# Binomial Both Logit Model - K fold validaton
# Set the number of folds (K)
num_folds <- 10
# Create an index vector for splitting
set.seed(123)  # for reproducibility
indices <- createFolds(insurance.data$insuranceclaim, k = num_folds, list = TRUE)
# Initialize a variable to store cross-validation results
cv_results <- data.frame(Accuracies = double(num_folds))
# Perform K-Fold Cross-Validation
for (i in 1:num_folds) {
# Split the data into training and testing sets
train_data <- insurance.data[-indices[[i]], ]
test_data <- insurance.data[indices[[i]], ]
# Fit your model on the training data
model <- step(full.logit,
direction="both",trace=0, data = train_data)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data,  type="response")
(table.full <- table(predictions > 0.5, test_data$insuranceclaim))
acc <- round((sum(diag(table.full))/sum(table.full))*100,2)
# Store the RMSE in the results dataframe
cv_results$Accuracies[i] <- acc
}
# Display cross-validation results
print(cv_results)
cv_results$Accuracies <- as.numeric(cv_results$Accuracies)
mean_accuracy <- mean(cv_results$Accuracies, na.rm = TRUE)
mean_accuracy
# Binomial Logit Model - K fold validation
# Probit Full Model
full.probit <- glm(insuranceclaim ~ . ,data = insurance.data.train ,
family = binomial(link = "probit"))
summary(full.probit)
car::qqPlot(residuals(full.probit), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(full.probit))
#train data accuracy
pred.both <- predict(full.probit, newdata = insurance.data.train, type="response")
(table.both <- table(pred.both > 0.5, insurance.data.train$insuranceclaim))
(accuracy.both.forward <- round((sum(diag(table.both.forward))/sum(table.both.forward))*100,2))
#test data accuracy
pred.both <- predict(full.probit, newdata = insurance.data.test, type="response")
(table.both <- table(pred.both > 0.5, insurance.data.test$insuranceclaim))
(accuracy.both.forward <- round((sum(diag(table.both.forward))/sum(table.both.forward))*100,2))
# Probit Model
full.predictors.probit <- glm(insuranceclaim ~ age + bmi + children + smoker ,data = insurance.data.train ,
family = binomial(link = "probit"))
summary(full.predictors.probit)
car::qqPlot(residuals(full.predictors.probit), main = NA, pch = 19, col = 2, cex = 0.7)
shapiro.test(residuals(full.predictors.probit))
pred.both.train.probit <- predict(full.predictors.probit, newdata = insurance.data.train, type="response")
(table.both <- table(pred.both.train.probit > 0.5, insurance.data.train$insuranceclaim))
(accuracy.both.forward <- round((sum(diag(table.both.forward))/sum(table.both.forward))*100,2))
pred.both.test.probit <- predict(full.predictors.probit, newdata = insurance.data.test, type="response")
(table.both <- table(pred.both.test.probit > 0.5, insurance.data.test$insuranceclaim))
(accuracy.both.forward <- round((sum(diag(table.both.forward))/sum(table.both.forward))*100,2))
par(mfrow = c(1,2))
roc.both <- roc(insurance.data.train$insuranceclaim ~ pred.both.train.probit, plot = TRUE,
legacy.axes = TRUE, print.auc = TRUE)
roc.both <- roc(insurance.data.test$insuranceclaim ~ pred.both.test.probit, plot = TRUE,
legacy.axes = TRUE, print.auc = TRUE)
# Binomial Full Probit Model - K fold validaton
# Set the number of folds (K)
num_folds <- 10
# Create an index vector for splitting
set.seed(123)  # for reproducibility
indices <- createFolds(insurance.data$insuranceclaim, k = num_folds, list = TRUE)
# Initialize a variable to store cross-validation results
cv_results <- data.frame(Accuracies = double(num_folds))
# Perform K-Fold Cross-Validation
for (i in 1:num_folds) {
# Split the data into training and testing sets
train_data <- insurance.data[-indices[[i]], ]
test_data <- insurance.data[indices[[i]], ]
# Fit your model on the training data
model <- glm(insuranceclaim ~ . ,data = train_data,
family = binomial(link = "probit"))
# Make predictions on the test data
predictions <- predict(model, newdata = test_data,  type="response")
(table.full <- table(predictions > 0.5, test_data$insuranceclaim))
acc <- round((sum(diag(table.full))/sum(table.full))*100,2)
# Store the RMSE in the results dataframe
cv_results$Accuracies[i] <- acc
}
# Display cross-validation results
print(cv_results)
cv_results$Accuracies <- as.numeric(cv_results$Accuracies)
mean_accuracy <- mean(cv_results$Accuracies, na.rm = TRUE)
mean_accuracy
# Binomial Logit Model - K fold validation
# Classification and Regression Trees
insurance.data.dup <- read.csv("~/Documents/GitHub/GitHub/Insurance-Claim-Prediction/data/insurance.csv")
insurance.data <- insurance.data.dup
set.seed(12345)
train.prop <- 0.80
strats <- insurance.data$insuranceclaim
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr,
function(x) sample(x, length(x)*train.prop)))))
insurance.data.train <- insurance.data[idx, ]
insurance.data.test <- insurance.data[-idx, ]
table(insurance.data.train$insuranceclaim)/nrow(insurance.data.train)
fit.allp <- rpart(insuranceclaim ~., method = "class", data = insurance.data.train,
control = rpart.control(minsplit = 1, cp = 0.001))
summary(fit.allp)
(rootnode_err <- sum(bank.data.train$y==1)/nrow(bank.data.train))
(rootnode_err <- sum(insurance.data.train$insuranceclaim==1)/nrow(insurance.data.train))
printcp(fit.allp)
max(fit.allp$cptable[,"nsplit"])
min(fit.allp$cptable[,"nsplit"])
plotcp(fit.allp)
(cp= fit.allp$cptable[which.min(fit.allp$cptable[, "xerror"]), "CP"])
(xerr = fit.allp$cptable[which.min(fit.allp$cptable[, "xerror"]), "xerror"])
rpart.plot(fit.allp, extra = "auto")
test_df <- data.frame(actual = insurance.data.test$insuranceclaim, pred = NA)
test_df$pred <- predict(fit.allp, newdata = insurance.data.test, type = "class")
(conf_matrix_base <- table(test_df$pred, test_df$actual)) #confusion matrix
sensitivity(conf_matrix_base)
specificity(conf_matrix_base)
(mis.rate <- conf_matrix_base[1, 2] +
conf_matrix_base[2, 1])/sum(conf_matrix_base)
#Hyper Parameter Tuning
fit.allf <- rpart(insuranceclaim ~., method = "class", data = insurance.data.train,
control = rpart.control(cp = 0.0001))
plotcp(fit.allp)
test_df <- data.frame(actual = insurance.data.test$insuranceclaim, pred = NA)
test_df$pred <- predict(fit.allp, newdata = insurance.data.test, type = "class")
(conf_matrix_base <- table(test_df$pred, test_df$actual)) #confusion matrix
sensitivity(conf_matrix_base)
specificity(conf_matrix_base)
(mis.rate <- conf_matrix_base[1, 2] +
conf_matrix_base[2, 1])/sum(conf_matrix_base)
fit.allf <- rpart(insuranceclaim ~., method = "class", data = insurance.data.train,
control = rpart.control(cp = 0.1))
plotcp(fit.allp)
test_df <- data.frame(actual = insurance.data.test$insuranceclaim, pred = NA)
test_df$pred <- predict(fit.allp, newdata = insurance.data.test, type = "class")
(conf_matrix_base <- table(test_df$pred, test_df$actual)) #confusion matrix
sensitivity(conf_matrix_base)
specificity(conf_matrix_base)
(mis.rate <- conf_matrix_base[1, 2] +
conf_matrix_base[2, 1])/sum(conf_matrix_base)
#Prune the tree
pfit.allp <- prune(fit.allp, cp =
fit.allp$cptable[which.min(fit.allp$cptable[, "xerror"]), "CP"])
rpart.plot(pfit.allp, extra = "auto")
summary(pfit.allp)
#Measures of Predictive Performance
rootnode_err <- sum(insurance.data.train$insuranceclaim==1)/nrow(insurance.data.train)
prelerr = pfit.allp$cptable[which.min(pfit.allp$cptable[, "rel error"]), "rel error"]
(presub.err_rate <- rootnode_err*prelerr)
rootnode_err <- sum(insurance.data.train$insuranceclaim==1)/nrow(insurance.data.train)
pxerr = pfit.allp$cptable[which.min(pfit.allp$cptable[, "xerror"]), "xerror"]
(pcv.err_rate <- rootnode_err*pxerr)
test_df <- data.frame(actual = insurance.data.test$insuranceclaim, pred = NA)
test_df$pred <- predict(pfit.allp, newdata = insurance.data.test, type = "class")
(conf_matrix_pruned_tree <-
table(test_df$pred, test_df$actual)) #confusion matrix
sensitivity(conf_matrix_pruned_tree)
specificity(conf_matrix_pruned_tree)
# Missclassification error rate:
(conf_matrix_pruned_tree[1, 2] +
conf_matrix_pruned_tree[2, 1])/sum(conf_matrix_pruned_tree)
# Random Forest
fit.rf.ranger <- ranger(insuranceclaim ~ ., data = insurance.data.train,
importance = 'impurity', mtry = 3)
print(fit.rf.ranger)
(v1 <- vi(fit.rf.ranger))
vip(v1)
# Assuming your predictions are stored in the variable 'pred'
pred <- predict(fit.rf.ranger, data = insurance.data.test)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.test$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
conf_matrix_rf <- table(test_df$pred, test_df$actual)
# Display the confusion matrix
print(conf_matrix_rf)
library(ggplot2)
library(caret)
library(dplyr)
library(vip)
library(ranger)
library(rpart)
library(rpart.plot)
library(pROC)
library(xgboost)
library(Matrix)
options(warn = -1)
library(ggplot2)
library(caret)
library(dplyr)
library(vip)
library(ranger)
library(rpart)
library(rpart.plot)
library(pROC)
library(xgboost)
library(Matrix)
options(warn = -1)
# Measure prediction accuracy on test data
(tab1<-table(bank.data.test.gbm,prediction))
# Transform the predictor matrix using dummy (or indicator or one-hot) encoding
matrix_predictors.train <-
as.matrix(sparse.model.matrix(y ~., data = insurance.data.train))[, -1]
# Transform the predictor matrix using dummy (or indicator or one-hot) encoding
matrix_predictors.train <-
as.matrix(sparse.model.matrix(insuranceclaim ~., data = insurance.data.train))[, -1]
matrix_predictors.test <-
as.matrix(sparse.model.matrix(insuranceclaim ~., data = insurance.data.test))[, -1]
# Train dataset
pred.train.gbm <- data.matrix(matrix_predictors.train) # predictors only
#convert factor to numeric
insurance.data.train.gbm <- as.numeric(as.character(insurance.data.train$insuranceclaim))
dtrain <- xgb.DMatrix(data = pred.train.gbm, label = insurance.data.train.gbm)
# Test dataset
pred.test.gbm <- data.matrix(matrix_predictors.test) # predictors only
#convert factor to numeric
insurance.data.test.gbm <- as.numeric(as.character(insurance.data.test$insuranceclaim))
dtest <- xgb.DMatrix(data = pred.test.gbm, label = insurance.data.test.gbm)
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 2, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
print(head(prediction))
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
sum(diag(tab))/sum(tab)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
print(head(prediction))
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
print(head(prediction))
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 2, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 8, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
# 10 rounds from 2
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
# 20 rounds
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
# 15 rounds
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
xgb.plot_importance(model.xgb)
xgb.importance.plot(model.xgb)
