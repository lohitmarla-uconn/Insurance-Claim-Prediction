test_df <- data.frame(actual = insurance.data.test$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
conf_matrix_rf <- table(test_df$pred, test_df$actual)
# Display the confusion matrix
print(conf_matrix_rf)
# Sensitivity
sensitivity(conf_matrix_rf)
# Specificity
specificity(conf_matrix_rf)
# Missclassification error rate:
(conf_matrix_rf[1,2] + conf_matrix_rf[2,1])/sum(conf_matrix_rf)
TP <- conf_matrix_rf[2, 2]  # True Positives
TN <- conf_matrix_rf[1, 1]  # True Negatives
FP <- conf_matrix_rf[1, 2]  # False Positives
FN <- conf_matrix_rf[2, 1]  # False Negatives
# Calculate Accuracy
accuracy <- (TP + TN) / (TP + TN + FP + FN)
# Print the accuracy
print(paste("Accuracy:", round(accuracy, 4)))
# 10 rounds from 2
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab1)
specificity(tab1)
sum(diag(tab1))/sum(tab1)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sensitivity(tab)
specificity(tab)
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sum(diag(tab1))/sum(tab1)
sensitivity(tab)
specificity(tab)
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab)
specificity(tab)
sum(diag(tab))/sum(tab)
# 10 rounds from 2
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab1)
specificity(tab1)
sum(diag(tab1))/sum(tab1)
# 15 rounds
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb.15 <- xgb.train(param, dtrain, nrounds = 15, watchlist)
pred.y.train <- predict(model.xgb.15, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb.15, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab1)
specificity(tab1)
sum(diag(tab1))/sum(tab1)
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c("CART", "Random Forest", "Random Forest", "XGBoost - 2 Rounds", "XGBoost - 10 Rounds", "XGBoost - 15 Rounds"),
Test_Accuracy    = c(0.82, 0.98, 0.98, 0.92, 0.88, 0.89),
Test_sensitivity = c(0.97, 0.98, 0.98, 0.90, 0.84, 0.90),
Test_specificity = c(0.97, 0.98, 0.92, 0.93, 0.90, 0.89)
)
# Print the table using kable
knitr::kable(df_model_results, caption = "Model Performance Results")
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c( "Logit - Full", "Logit - Both", "Probit"),
Train_Accuracy = c(88.49, 87.37, 86.83),
Test_Accuracy = c(88.06, 85.45, 86.31),
AIC = c(634.5677, 772.4223, 786.43),
BIC = c(704.0126, 817.1926, 862.65),
ROC = c(0.916, 0.913, 0.925)
)
# Print the table using kable
knitr::kable(df_model_results, caption = "Model Performance Results")
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c("CART", "Random Forest", "Random Forest Reduced Predictors", "XGBoost - 2 Rounds", "XGBoost - 10 Rounds", "XGBoost - 15 Rounds"),
Test_Accuracy    = c(0.82, 0.98, 0.98, 0.92, 0.88, 0.89),
Test_sensitivity = c(0.97, 0.98, 0.98, 0.90, 0.84, 0.90),
Test_specificity = c(0.97, 0.98, 0.92, 0.93, 0.90, 0.89)
)
# Print the table using kable
knitr::kable(df_model_results, caption = "Model Performance Results")
# Assuming df_model_results is your data frame containing the model results
df_model_results_logit_probit <- data.frame(
Model = c( "Logit - Full", "Logit - Both", "Probit"),
Train_Accuracy = c(88.49, 87.37, 86.83),
Test_Accuracy = c(88.06, 85.45, 86.31),
AIC = c(634.5677, 772.4223, 786.43),
BIC = c(704.0126, 817.1926, 862.65),
ROC = c(0.916, 0.913, 0.925)
)
# Print the table using kable
knitr::kable(df_model_results_logit_probit, caption = "Model Performance Results")
# 15 rounds
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb.15 <- xgb.train(param, dtrain, nrounds = 15, watchlist)
pred.y.train <- predict(model.xgb.15, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sensitivity(tab)
specificity(tab)
sum(diag(tab))/sum(tab)
pred.y = predict(model.xgb.15, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab1)
specificity(tab1)
sum(diag(tab1))/sum(tab1)
# 15 rounds
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb.15 <- xgb.train(param, dtrain, nrounds = 15, watchlist)
pred.y.train <- predict(model.xgb.15, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
pred.y = predict(model.xgb.15, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab1)
specificity(tab1)
sum(diag(tab1))/sum(tab1)
# 10 rounds from 2
watchlist <- list(train = dtrain, test = dtest)
param <- list(max_depth = 2, eta = 1, nthread = 2,
objective = "binary:logistic", eval_metric = "auc")
model.xgb <- xgb.train(param, dtrain, nrounds = 10, watchlist)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
pred.y = predict(model.xgb, pred.test.gbm)
prediction <- as.numeric(pred.y > 0.5)
# Measure prediction accuracy on test data
(tab1<-table(insurance.data.test.gbm,prediction))
sensitivity(tab1)
specificity(tab1)
sum(diag(tab1))/sum(tab1)
pred.y.train <- predict(model.xgb, pred.train.gbm)
prediction.train <- as.numeric(pred.y.train > 0.5)
# Measure prediction accuracy on train data
(tab<-table(insurance.data.train.gbm, prediction.train))
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
# dropped the sex, age and region column
fit.rf.ranger <- ranger(insuranceclaim ~ bmi+children+smoker+charges, data = insurance.data.train,
importance = 'impurity', mtry = 3)
print(fit.rf.ranger)
(v1 <- vi(fit.rf.ranger))
vip_plot <- vip(v1)
vip_plot + ggtitle("Variable Importance Plot - Bmi, Children, Smoker, Charges")
pred <- predict(fit.rf.ranger, data = insurance.data.test)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.test$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
conf_matrix_rf <- table(test_df$pred, test_df$actual)
# Display the confusion matrix
print(conf_matrix_rf)
# Sensitivity
sensitivity(conf_matrix_rf)
# Specificity
specificity(conf_matrix_rf)
# Missclassification error rate:
(conf_matrix_rf[1,2] + conf_matrix_rf[2,1])/sum(conf_matrix_rf)
TP <- conf_matrix_rf[2, 2]  # True Positives
TN <- conf_matrix_rf[1, 1]  # True Negatives
FP <- conf_matrix_rf[1, 2]  # False Positives
FN <- conf_matrix_rf[2, 1]  # False Negatives
# Calculate Accuracy
accuracy <- (TP + TN) / (TP + TN + FP + FN)
# Print the accuracy
print(paste("Accuracy:", round(accuracy, 4)))
pred <- predict(fit.rf.ranger, data = insurance.data.train)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.train$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
tab <- table(test_df$pred, test_df$actual)
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c("CART", "Random Forest", "Random Forest Reduced Predictors", "XGBoost - 2 Rounds", "XGBoost - 10 Rounds", "XGBoost - 15 Rounds"),
Test_Accuracy    = c(0.82, 0.98, 0.98, 0.92, 0.88, 0.89),
Test_sensitivity = c(0.97, 0.98, 0.98, 0.90, 0.84, 0.90),
Test_specificity = c(0.97, 0.98, 0.92, 0.93, 0.90, 0.89),
Train_Accuracy    = c(, , , 0.92, 0.92, 0.94),
Train_sensitivity = c(, , , 0.90, 0.90, 0.93),
Train_specificity = c(, , , 0.93, 0.93, 0.94)
)
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c("CART", "Random Forest", "Random Forest Reduced Predictors", "XGBoost - 2 Rounds", "XGBoost - 10 Rounds", "XGBoost - 15 Rounds"),
Test_Accuracy    = c(0.82, 0.98, 0.98, 0.92, 0.88, 0.89),
Test_sensitivity = c(0.97, 0.98, 0.98, 0.90, 0.84, 0.90),
Test_specificity = c(0.97, 0.98, 0.92, 0.93, 0.90, 0.89),
Train_Accuracy    = c("","" ,"" , 0.92, 0.92, 0.94),
Train_sensitivity = c("", "", "", 0.90, 0.90, 0.93),
Train_specificity = c("", "", "", 0.93, 0.93, 0.94)
)
# Print the table using kable
knitr::kable(df_model_results, caption = "Model Performance Results")
# dropped the sex, age and region column
fit.rf.ranger <- ranger(insuranceclaim ~ bmi+children+smoker+charges, data = insurance.data.train,
importance = 'impurity', mtry = 3)
print(fit.rf.ranger)
(v1 <- vi(fit.rf.ranger))
vip_plot <- vip(v1)
vip_plot + ggtitle("Variable Importance Plot - Bmi, Children, Smoker, Charges")
pred <- predict(fit.rf.ranger, data = insurance.data.test)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.test$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
conf_matrix_rf <- table(test_df$pred, test_df$actual)
# Display the confusion matrix
print(conf_matrix_rf)
# Sensitivity
sensitivity(conf_matrix_rf)
# Specificity
specificity(conf_matrix_rf)
# Missclassification error rate:
(conf_matrix_rf[1,2] + conf_matrix_rf[2,1])/sum(conf_matrix_rf)
TP <- conf_matrix_rf[2, 2]  # True Positives
TN <- conf_matrix_rf[1, 1]  # True Negatives
FP <- conf_matrix_rf[1, 2]  # False Positives
FN <- conf_matrix_rf[2, 1]  # False Negatives
# Calculate Accuracy
accuracy <- (TP + TN) / (TP + TN + FP + FN)
# Print the accuracy
print(paste("Accuracy:", round(accuracy, 4)))
pred <- predict(fit.rf.ranger, data = insurance.data.train)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.train$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
tab <- table(test_df$pred, test_df$actual)
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
# dropped the sex, age and region column
fit.rf.ranger <- ranger(insuranceclaim ~ bmi+children+smoker+charges, data = insurance.data.train,
importance = 'impurity', mtry = 3)
print(fit.rf.ranger)
(v1 <- vi(fit.rf.ranger))
vip_plot <- vip(v1)
vip_plot + ggtitle("Variable Importance Plot - Bmi, Children, Smoker, Charges")
pred <- predict(fit.rf.ranger, data = insurance.data.test)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.test$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
conf_matrix_rf <- table(test_df$pred, test_df$actual)
# Display the confusion matrix
print(conf_matrix_rf)
# Sensitivity
sensitivity(conf_matrix_rf)
# Specificity
specificity(conf_matrix_rf)
# Missclassification error rate:
(conf_matrix_rf[1,2] + conf_matrix_rf[2,1])/sum(conf_matrix_rf)
TP <- conf_matrix_rf[2, 2]  # True Positives
TN <- conf_matrix_rf[1, 1]  # True Negatives
FP <- conf_matrix_rf[1, 2]  # False Positives
FN <- conf_matrix_rf[2, 1]  # False Negatives
# Calculate Accuracy
accuracy <- (TP + TN) / (TP + TN + FP + FN)
# Print the accuracy
print(paste("Accuracy:", round(accuracy, 4)))
pred <- predict(fit.rf.ranger, data = insurance.data.train)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.train$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
tab <- table(test_df$pred, test_df$actual)
tab
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
# dropped the sex and region column
fit.rf.ranger <- ranger(insuranceclaim ~ bmi+children+age+smoker+charges, data = insurance.data.train,
importance = 'impurity', mtry = 3)
print(fit.rf.ranger)
(v1 <- vi(fit.rf.ranger))
vip_plot <- vip(v1)
vip_plot + ggtitle("Variable Importance Plot - Bmi, Children, Smoker, Charges, Age")
pred <- predict(fit.rf.ranger, data = insurance.data.test)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.test$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
conf_matrix_rf <- table(test_df$pred, test_df$actual)
# Display the confusion matrix
print(conf_matrix_rf)
# Sensitivity
sensitivity(conf_matrix_rf)
# Specificity
specificity(conf_matrix_rf)
# Missclassification error rate:
(conf_matrix_rf[1,2] + conf_matrix_rf[2,1])/sum(conf_matrix_rf)
pred <- predict(fit.rf.ranger, data = insurance.data.train)
# Create a data frame with actual and predicted values
test_df <- data.frame(actual = insurance.data.train$insuranceclaim,
pred = ifelse(pred$predictions > 0.5, 1, 0))
# Create a confusion matrix
tab <- table(test_df$pred, test_df$actual)
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
#Hyper Parameter Tuning
fit.allf <- rpart(insuranceclaim ~., method = "class", data = insurance.data.train,
control = rpart.control(cp = 0.0001))
plotcp(fit.allp)
test_df <- data.frame(actual = insurance.data.test$insuranceclaim, pred = NA)
test_df$pred <- predict(fit.allp, newdata = insurance.data.test, type = "class")
(conf_matrix_base <- table(test_df$pred, test_df$actual)) #confusion matrix
sensitivity(conf_matrix_base)
specificity(conf_matrix_base)
(mis.rate <- conf_matrix_base[1, 2] +
conf_matrix_base[2, 1])/sum(conf_matrix_base)
test_df <- data.frame(actual = insurance.data.train$insuranceclaim, pred = NA)
test_df$pred <- predict(fit.allp, newdata = insurance.data.train, type = "class")
(tab <- table(test_df$pred, test_df$actual)) #confusion matrix
sum(diag(tab))/sum(tab)
sensitivity(tab)
specificity(tab)
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c("CART", "Random Forest", "Random Forest Reduced Predictors", "XGBoost - 2 Rounds", "XGBoost - 10 Rounds", "XGBoost - 15 Rounds"),
Test_Accuracy    = c(0.82, 0.98, 0.98, 0.92, 0.88, 0.89),
Test_sensitivity = c(0.97, 0.98, 0.98, 0.90, 0.84, 0.90),
Test_specificity = c(0.97, 0.98, 0.92, 0.93, 0.90, 0.89),
Train_Accuracy    = c(1,1 ,1 , 0.92, 0.92, 0.94),
Train_sensitivity = c(1, 1, 1, 0.90, 0.90, 0.93),
Train_specificity = c(1, 1, 1, 0.93, 0.93, 0.94)
)
# Print the table using kable
knitr::kable(df_model_results, caption = "Model Performance Results")
# Assuming df_model_results is your data frame containing the model results
df_model_results_logit_probit <- data.frame(
Model = c( "Logit - Full", "Logit - Both", "Probit"),
Train_Accuracy = c(88.49, 87.37, 86.83),
Test_Accuracy = c(88.06, 85.45, 86.31),
AIC = c(634.5677, 772.4223, 786.43),
BIC = c(704.0126, 817.1926, 862.65),
ROC = c(0.916, 0.913, 0.925)
)
# Print the table using kable
knitr::kable(df_model_results_logit_probit, caption = "Bi-nomial Model Performance Results")
# Assuming df_model_results is your data frame containing the model results
df_model_results <- data.frame(
Model = c("CART", "Random Forest", "Random Forest Reduced Predictors", "XGBoost - 2 Rounds", "XGBoost - 10 Rounds", "XGBoost - 15 Rounds"),
Test_Accuracy    = c(0.82, 0.98, 0.98, 0.92, 0.88, 0.89),
Test_sensitivity = c(0.97, 0.98, 0.98, 0.90, 0.84, 0.90),
Test_specificity = c(0.97, 0.98, 0.92, 0.93, 0.90, 0.89),
Train_Accuracy    = c(1,1 ,1 , 0.92, 0.92, 0.94),
Train_sensitivity = c(1, 1, 1, 0.90, 0.90, 0.93),
Train_specificity = c(1, 1, 1, 0.93, 0.93, 0.94)
)
# Print the table using kable
knitr::kable(df_model_results, caption = "Decision Trees - Model Performance Results")
# Assuming df_model_results is your data frame containing the model results
df_model_results_logit_probit <- data.frame(
Model = c( "Logit - Full", "Logit - Both", "Probit"),
Train_Accuracy = c(88.49, 87.37, 86.83),
Test_Accuracy = c(88.06, 85.45, 86.31),
AIC = c(779.29, 772.4223, 786.43),
BIC = c(704.0126, 817.1926, 862.65),
ROC = c(0.916, 0.913, 0.925)
)
# Print the table using kable
knitr::kable(df_model_results_logit_probit, caption = "Bi-nomial Model Performance Results")
suppressWarnings({
library(ggplot2)
library(caret)
library(dplyr)
library(vip)
library(ranger)
library(rpart)
library(rpart.plot)
library(pROC)
library(xgboost)
library(Matrix)
library(DiagrammeR)
library(knitr)
})
library(ggplot2)
library(caret)
library(dplyr)
library(vip)
library(ranger)
library(rpart)
library(rpart.plot)
library(pROC)
library(xgboost)
library(Matrix)
library(DiagrammeR)
library(knitr)
set.seed(123457)
train.prop <- 0.80
strats <- insurance.data$insuranceclaim
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr,
function(x) sample(x, length(x)*train.prop)))))
insurance.data.train <- insurance.data[idx, ]
insurance.data.test <- insurance.data[-idx, ]
#full binary logit model
full.logit <- glm(insuranceclaim ~ . ,data = insurance.data.train,
family = binomial(link = "logit"))
null.logit <- glm(insuranceclaim ~ 1 ,data = insurance.data.train,
family = binomial(link = "logit"))
both.logit <- step(null.logit, list(lower = formula(null.logit),
upper = formula(full.logit)),
direction = "both", trace = 0, data = insurance.data.train)
# Set up a 1x2 layout for side-by-side plots
par(mfrow = c(1, 2))
# Plot the first qqPlot
car::qqPlot(residuals(full.logit), main = NA, pch = 19, col = 2, cex = 0.7)
# Plot the second qqPlot
car::qqPlot(residuals(both.logit), main = NA, pch = 19, col = 2, cex = 0.7)
# Add a common title for the entire layout
title("Quantile-Quantile Plots for Residuals - Logit")
# Reset the layout to a single plot
par(mfrow = c(1, 1))
set.seed(123457)
train.prop <- 0.80
strats <- insurance.data$insuranceclaim
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr,
function(x) sample(x, length(x)*train.prop)))))
insurance.data.train <- insurance.data[idx, ]
insurance.data.test <- insurance.data[-idx, ]
#full binary logit model
full.logit <- glm(insuranceclaim ~ . ,data = insurance.data.train,
family = binomial(link = "logit"))
null.logit <- glm(insuranceclaim ~ 1 ,data = insurance.data.train,
family = binomial(link = "logit"))
both.logit <- step(null.logit, list(lower = formula(null.logit),
upper = formula(full.logit)),
direction = "both", trace = 0, data = insurance.data.train)
# Set up the layout
par(mfrow = c(1, 2), oma = c(0, 2, 0, 0))
# Plot the first qqPlot
car::qqPlot(residuals(full.logit), main = NA, pch = 19, col = 2, cex = 0.7)
# Plot the second qqPlot
car::qqPlot(residuals(both.logit), main = NA, pch = 19, col = 2, cex = 0.7)
# Add a common title for the entire layout
mtext("Quantile-Quantile Plots for Residuals - Logit", side = 3, outer = TRUE, line = -2)
# Reset the layout to a single plot
par(mfrow = c(1, 1), oma = c(0, 0, 0, 0))
